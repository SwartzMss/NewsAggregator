## 新闻去重方案概览

本方案希望在文章入库前识别「同一新闻在不同源重复发布」的情况，避免列表出现重复内容，同时保留多源引用信息，方便后续展示和分析。整体流程分为多层过滤，只有在判定不确定时才调用大模型。

### 目标
- 阻止重复新闻进入 `news.articles` 主表，或将它们与已有文章关联。
- 尽量保留其他来源的信息（发布时间、源站、原始 URL）。
- 控制成本与延迟，对高并发抓取保持可扩展性。

### 总体流程
1. **预处理：URL 与标题归一化**  
   - 去掉追踪参数（`utm_*` 等）、统一协议/域名、修剪空格和标点。  
   - 标题统一大小写、移除噪声字符（如站点前缀）。

2. **快速规则判定**  
   - 直接命中 `(feed_id, normalized_url)` 唯一约束 → 视为重复。  
   - URL 完全相同 → 视为重复。  
   - 发布时间差距超出阈值（如 >48 小时） → 视为不同。  
   - 标题完全一致且发布时间很接近 → 视为重复。

3. **轻量相似度 + DeepSeek 判定**  
   - 对标题做归一化后计算 Jaccard，阈值 ≥0.9 时直接判定重复（已在 fetcher 中实现）。  
   - 若与最近文章的相似度位于中间区间（例如 0.6–0.9），则调用 DeepSeek（`backend/src/fetcher/mod.rs`）对标题/摘要/发布时间进行语义比对；模型判定为重复时丢弃新文章。  
   - DeepSeek API 通过配置 `DEEPSEEK_API_KEY` 启用，结果包含原因/置信度，可在日志中追踪。

5. **数据落库策略**
   - 新文章成功写入后会记录主来源；被判定重复的条目仅写入来源表并附带触发原因。具体 schema 与 SQL 见 `docs/database.md`。

6. **观察与回溯**
   - 监控判定结果（重复/非重复的比例、模型调用次数、平均延迟）。
   - 对模型输出进行抽样人工复核，必要时调整阈值或 prompt。
   - 提供批处理脚本，对历史数据进行去重迁移。

### 精选排序的后续优化思路
- 维持当前点击量降序的主排序逻辑，同时只考虑最近 24 小时的文章。
- 引入来源权重：当点击量相同或接近时，可按照 `source_count`（`news.article_sources` 中关联的来源条数）进行次排序，让被多家源报道的新闻优先展示。
- 具体实现待数据结构和去重链路落地后再更新，包括如何在查询中统计来源数量、是否需要缓存或物化视图等。

### 实施建议
- **第一阶段**：先上线 URL 归一化 + `(feed_id, url)` 唯一约束 + 发布时间窗口，初步减少重复。
- **第二阶段**：实现标题归一化 + Jaccard/SimHash 等轻量规则过滤，对高优先级重复直接拦截。
- **第三阶段**：集成 DeepSeek 语义判定（已完成，需配置 `DEEPSEEK_API_KEY`）。
- **第四阶段（后续规划）**：扩展数据库结构（新增 `news.article_sources`、`canonical_article_id` 等）并调整入库流程、API 返回值。
- **展示层**：若启用引用表，在 API 返回时附带 `sources` 列表，展示“该新闻还来自哪些源站/时间”。  
- **后续优化**：记录模型答案，结合反馈不断调整 prompt 和阈值，必要时训练自定义分类器替换部分模型调用。

通过这种多层过滤机制，可以在成本可控的前提下显著降低跨源重复新闻的占比，同时保留业务上有价值的多源信息。
