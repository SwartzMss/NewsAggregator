## 新闻去重方案概览

本方案希望在文章入库前识别「同一新闻在不同源重复发布」的情况，避免列表出现重复内容，同时保留多源引用信息，方便后续展示和分析。整体流程分为多层过滤，只有在判定不确定时才调用大模型。

### 目标
- 阻止重复新闻进入 `news.articles` 主表，或将它们与已有文章关联。
- 尽量保留其他来源的信息（发布时间、源站、原始 URL）。
- 控制成本与延迟，对高并发抓取保持可扩展性。

### 总体流程
1. **预处理：URL 与标题归一化**  
   - 去掉追踪参数（`utm_*` 等）、统一协议/域名、修剪空格和标点。  
   - 标题统一大小写、移除噪声字符（如站点前缀）。

2. **快速规则判定**  
   - 直接命中 `(feed_id, normalized_url)` 唯一约束 → 视为重复。  
   - URL 完全相同 → 视为重复。  
   - 发布时间差距超出阈值（如 >48 小时） → 视为不同。  
   - 标题完全一致且发布时间很接近 → 视为重复。

3. **轻量相似度计算**  
   - 只在候选集中对标题/摘要做 TF-IDF、SimHash、Trigram 等快速相似度，或使用本地嵌入模型计算余弦相似。  
   - 设定双阈值：高于上限 → 判重复；低于下限 → 判不同；介于两者 → 进入下一层。
   - 候选集可通过时间窗口（如最近 24h）或向量索引取 Top-K，避免全库扫描。

4. **大模型判定（DeepSeek 等）**  
   - 对“灰区”候选，将标题、来源、发布时间、摘要等信息组装成 prompt，明确让模型回答“是否同一新闻”和理由。  
   - 模型只返回“是/否 + 简短说明”，便于机器解析。
   - 对结果做缓存：同一组合下次直接复用。

5. **数据落库策略**  
   - 若判定为全新新闻：照常插入 `news.articles`，并在 `news.article_sources`（待新增）记录当前来源。  
   - 若判定为重复：  
     - **方案 A**：不插入主表，只在 `news.article_sources` 中追加一条引用，指向已存在的主文章 ID。  
     - **方案 B**：保留重复记录，但将 `canonical_id` 指向主文章，并在展示时合并（实现成本更高）。  
   - 保留模型判定、相似度分数和关键字段，方便审计/回溯。

6. **观察与回溯**  
   - 监控判定结果（重复/非重复的比例、模型调用次数、平均延迟）。  
   - 对模型输出进行抽样人工复核，必要时调整阈值或 prompt。  
   - 提供批处理脚本，对历史数据进行去重迁移。

### 实施建议
- **第一阶段**：先上线 URL 归一化 + `(feed_id, url)` 唯一约束 + 发布时间窗口，初步减少重复。  
- **第二阶段**：加入轻量相似度（可以选 TF-IDF/SimHash）和引用表结构，先不调用大模型。  
- **第三阶段**：接入 DeepSeek 作为补充，只处理被标记为“高相似但不确定”的案例，并严格限流。  
- **展示层**：若启用引用表，在 API 返回时附带 `sources` 列表，展示“该新闻还来自哪些源站/时间”。  
- **后续优化**：记录模型答案，结合反馈不断调整 prompt 和阈值，必要时训练自定义分类器替换部分模型调用。

通过这种多层过滤机制，可以在成本可控的前提下显著降低跨源重复新闻的占比，同时保留业务上有价值的多源信息。
